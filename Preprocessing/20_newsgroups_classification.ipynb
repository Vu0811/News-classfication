{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4bcd9738",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 543
        },
        "id": "gObvzp1gGrxo",
        "outputId": "75655760-ee7c-4493-c3d3-28632fb0d3c4"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2f6dd633",
      "metadata": {
        "id": "346a169f"
      },
      "source": [
        "# Load data and visualize data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8e8227a9",
      "metadata": {
        "id": "d5abe9b0"
      },
      "source": [
        "Get the path of project"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bc9cb379",
      "metadata": {
        "id": "17fe496c"
      },
      "outputs": [],
      "source": [
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2a352ec9",
      "metadata": {
        "id": "4968bae3",
        "outputId": "f3eab717-6d07-4d1c-bd06-64382910d36a"
      },
      "outputs": [],
      "source": [
        "os.getcwd() "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "24bfa996",
      "metadata": {
        "id": "9906b572",
        "outputId": "e757d065-a49b-4489-ca59-d6b2ec159277"
      },
      "outputs": [],
      "source": [
        "project_root_path = os.getcwd()\n",
        "project_root_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "459c95c9",
      "metadata": {
        "id": "c1e5a001",
        "outputId": "71a9fdf4-14e6-4487-cedb-dc7a74189dc8"
      },
      "outputs": [],
      "source": [
        "data_root_path = project_root_path + \"\\\\_data\\\\\"\n",
        "data_root_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5718017a",
      "metadata": {
        "id": "f17ebd78"
      },
      "outputs": [],
      "source": [
        "# get the path of dataset\n",
        "data_path = data_root_path + \"20_newsgroups\\\\\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bbcfc78c",
      "metadata": {
        "id": "7164f131"
      },
      "source": [
        "Load data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "07bddcac",
      "metadata": {
        "id": "060afea2"
      },
      "outputs": [],
      "source": [
        "from time import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "483a1a66",
      "metadata": {
        "id": "38c01b1a",
        "outputId": "9b5baa3b-e76d-4559-a04d-b49fb1469fb7"
      },
      "outputs": [],
      "source": [
        "start_time = time()\n",
        "data = []\n",
        "labels = []\n",
        "labels_count = {}\n",
        "# list all folder from root_folder\n",
        "for category in os.listdir(data_path):\n",
        "    \n",
        "    # read each child folder in data folder\n",
        "    if category.lower() != '.ds_store':\n",
        "        # add label to dictationary\n",
        "        labels_count[category] = 0\n",
        "        for document in os.listdir(data_path + category):\n",
        "            # read text file in each folder\n",
        "            with open(data_path + category + \"/\" + document, \"r\", encoding=\"utf-8\", errors=\"ignore\") as textfile:\n",
        "                contents = textfile.read() \n",
        "                # add context to list data\n",
        "                data.append(contents)\n",
        "                # add label to list labels\n",
        "                labels.append(category)\n",
        "                # count number of label\n",
        "                labels_count[category] += 1\n",
        "print(\"Time to load data: \" + str(time() - start_time) + 's')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a69da210",
      "metadata": {
        "id": "c226a4d7"
      },
      "source": [
        "Load the dataset in DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c9e32f75",
      "metadata": {
        "id": "66538548"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e3a56f78",
      "metadata": {
        "id": "b95cc544",
        "outputId": "694c793c-010c-4202-caf1-9541019e5234"
      },
      "outputs": [],
      "source": [
        "df = pd.DataFrame()\n",
        "df['text'] = data\n",
        "df['label'] = labels\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "56bffda0",
      "metadata": {
        "id": "cf453b68"
      },
      "source": [
        "Count the number of each label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9e39c31c",
      "metadata": {
        "id": "8cb2c6cd",
        "outputId": "415c3986-501a-4efd-92ed-d494ec24777e"
      },
      "outputs": [],
      "source": [
        "# count the number of each label\n",
        "df['label'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2343c0d9",
      "metadata": {
        "id": "a69ace43",
        "outputId": "80ff8c73-e7b1-447d-9693-783b4c932551"
      },
      "outputs": [],
      "source": [
        "# number of targets\n",
        "df['label'].nunique()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b9fe37d4",
      "metadata": {
        "id": "28bffb44"
      },
      "source": [
        "View in Barchart"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1cf08117",
      "metadata": {
        "id": "e6c91631",
        "outputId": "93312d15-8b2a-4664-8718-68da161794c3"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Figure Size\n",
        "fig, ax = plt.subplots(figsize =(16, 9))\n",
        "# creating the bar plot\n",
        "ax.barh(list(labels_count.keys()), list(labels_count.values()), color='grey')\n",
        "# Add x, y gridlines# Add annotation to bars\n",
        "for i in ax.patches:\n",
        "    plt.text(i.get_width()+0.2, i.get_y()+0.5,\n",
        "             str(round((i.get_width()), 2)),\n",
        "             fontsize = 10, fontweight ='bold',\n",
        "             color ='grey')\n",
        "ax.set_title(\"Number of data in different News type\",loc ='left')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "38c013c3",
      "metadata": {
        "id": "9b198830"
      },
      "source": [
        "Data is balanced\n",
        "We are going to make a number of words column in which there is the number of words in a particular text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "11eb84c7",
      "metadata": {
        "id": "3017e387",
        "outputId": "a8f2b28e-4790-43ea-84c2-7dbed099b813"
      },
      "outputs": [],
      "source": [
        "# Count the number of words\n",
        "df['number_of_words'] = df['text'].apply(lambda x:len(str(x).split()))\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4dba504c",
      "metadata": {
        "id": "b4b66aaa"
      },
      "source": [
        "Check the basic stats of number of words, like maximum, minimum, average number of words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "694d7777",
      "metadata": {
        "id": "369662ac",
        "outputId": "70650931-b2eb-4c4e-f3f7-182f13ec047e"
      },
      "outputs": [],
      "source": [
        "df['number_of_words'].describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ce85a5eb",
      "metadata": {
        "id": "a9e5e862"
      },
      "source": [
        "So maximum number of words text is belongs to electronics category.\n",
        "In our dataset we have some rows where there are no text at all i.e. the number of words is 0.We will drop those rows"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "942bb061",
      "metadata": {
        "id": "15f86762",
        "outputId": "931473e9-37c7-4df9-ef6c-390267f27845"
      },
      "outputs": [],
      "source": [
        "# number of rows with text length = 0\n",
        "no_text = df[df['number_of_words'] == 0]\n",
        "print(len(no_text))\n",
        "\n",
        "# drop these rows\n",
        "df.drop(no_text.index,inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "21267887",
      "metadata": {
        "id": "d5eddf1e"
      },
      "source": [
        "### Visualize the frequency distribution of number of words for each text extracted"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "61c8576e",
      "metadata": {
        "id": "4a996d1f",
        "outputId": "73f19574-fed3-4725-e3f4-fb1053d5be29"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "\n",
        "plt.style.use('ggplot')\n",
        "plt.figure(figsize=(12,6))\n",
        "sns.distplot(df['number_of_words'], kde = False, color=\"red\", bins=200)\n",
        "plt.title(\"Frequency distribution of number of words for each text extracted\", size=20)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f0316cf2",
      "metadata": {
        "id": "4e5744ec"
      },
      "source": [
        "# Data Pre-Processing"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "05faf3c7",
      "metadata": {
        "id": "8c663152"
      },
      "source": [
        "### Clean the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "42bd79b0",
      "metadata": {
        "id": "20e69123"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import string"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "311da861",
      "metadata": {
        "id": "d7ab7821"
      },
      "outputs": [],
      "source": [
        "def clean_header(text):\n",
        "    text = re.sub(r'(From:\\s+[^\\n]+\\n)', '', text)\n",
        "    text = re.sub(r'(Subject:[^\\n]+\\n)', '', text)\n",
        "    text = re.sub(r'(([\\sA-Za-z0-9\\-]+)?[A|a]rchive-name:[^\\n]+\\n)', '', text)\n",
        "    text = re.sub(r'(Last-modified:[^\\n]+\\n)', '', text)\n",
        "    text = re.sub(r'(Version:[^\\n]+\\n)', '', text)\n",
        "\n",
        "    return text\n",
        "\n",
        "def clean_text(text):\n",
        "    # remove header\n",
        "    text = clean_header(text)\n",
        "    # lower text\n",
        "    text = text.lower()\n",
        "    # remove text in square brackets\n",
        "    text = re.sub('\\[.*?\\]', ' ', text)\n",
        "    # remove link\n",
        "    text = re.sub('https?://\\S+|www\\.\\S+', ' ', text)\n",
        "    # remove email\n",
        "    text = re.sub(r'([a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+)',' ', text)\n",
        "    # remove HTML tag\n",
        "    text = re.sub('<.*?>+', ' ', text)\n",
        "    # remove punctuation\n",
        "    text = re.sub('[%s]' % re.escape(string.punctuation), ' ', text)\n",
        "    # remove special characters\n",
        "    text = re.sub('[^a-zA-Z]', ' ', text)\n",
        "    # remove empty line\n",
        "    text = re.sub('\\n', ' ', text)\n",
        "    # remove words containing numbers.\n",
        "    text = re.sub('\\w*\\d\\w*', ' ', text)\n",
        "    # remove extra whitespaces\n",
        "    text = re.sub(' +', ' ', text)\n",
        "    # remove single character\n",
        "    text = ' '.join([word for word in text.split() if len(word) > 1])\n",
        "    return text.strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "11cfddf6",
      "metadata": {
        "id": "5d83c358",
        "outputId": "8e8ad3bf-8e88-4107-fda1-76b79816dbcf"
      },
      "outputs": [],
      "source": [
        "start_time = time()\n",
        "df['cleaned_text'] = df['text'].apply(clean_text)\n",
        "print(\"Time to process data: \" + str(time() - start_time) + 's')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ff83a393",
      "metadata": {
        "id": "8ce60493",
        "outputId": "6f0f7064-92fd-4d04-ebd4-3ef1637f7d81"
      },
      "outputs": [],
      "source": [
        "df['cleaned_text'].head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b3fadebd",
      "metadata": {
        "id": "6b46be8d"
      },
      "source": [
        "Count the number of words again"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "db90e0de",
      "metadata": {
        "id": "f718a7f6",
        "outputId": "ecdba10a-8d23-43b4-f698-6178f68d7948"
      },
      "outputs": [],
      "source": [
        "df['number_of_cleaned_words'] = df['cleaned_text'].apply(lambda x:len(str(x).split()))\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a57ac994",
      "metadata": {
        "id": "4876cd89",
        "outputId": "963d73fe-aff1-46fe-b85e-15ca53518e8a"
      },
      "outputs": [],
      "source": [
        "df['number_of_cleaned_words'].describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0f4b3975",
      "metadata": {
        "id": "3cbf5b25",
        "outputId": "c94bf1a7-ce67-40da-d286-69f8d8a7c61b"
      },
      "outputs": [],
      "source": [
        "# number of rows with text length = 0\n",
        "no_text = df[df['number_of_cleaned_words']==0]\n",
        "print(len(no_text))\n",
        "\n",
        "# drop these rows\n",
        "df.drop(no_text.index,inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e031b4f3",
      "metadata": {
        "id": "RHueTS-RZJyW"
      },
      "source": [
        "### Remove stop words"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8f0a9ec2",
      "metadata": {
        "id": "26e20f19"
      },
      "source": [
        "Let's convert our cleaned text into tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0535e67e",
      "metadata": {
        "id": "75295b94",
        "outputId": "2fa22e1c-3eea-4796-cf63-7d19978c0f79"
      },
      "outputs": [],
      "source": [
        "df['tokens'] = df['cleaned_text'].apply(lambda x: x.split())\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9eca04e3",
      "metadata": {
        "id": "5fc3a401"
      },
      "source": [
        "Stopwords are those english words which do not add much meaning to a sentence.\n",
        "They are very commonly used words and we do not required those words. \n",
        "So we can remove those stopwords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1723bb77",
      "metadata": {
        "code_folding": [],
        "id": "7c77baee",
        "outputId": "13e936bd-5799-4316-bf22-c814ffa80d1a"
      },
      "outputs": [],
      "source": [
        "# stopwords\n",
        "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
        "print(ENGLISH_STOP_WORDS)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d94d8d90",
      "metadata": {
        "id": "092ec621"
      },
      "source": [
        "Check number of stopwords in library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c5420ffd",
      "metadata": {
        "id": "6ca9225b",
        "outputId": "db98b37c-07eb-4d2b-c053-409efff3de9f"
      },
      "outputs": [],
      "source": [
        "len(ENGLISH_STOP_WORDS)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a18e2310",
      "metadata": {
        "id": "41a57c04"
      },
      "source": [
        "We are going to remove the stopwords from the text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a5269c45",
      "metadata": {
        "id": "7a9dc26f",
        "outputId": "7c66702c-df91-44a3-9d76-80a9a7c83874"
      },
      "outputs": [],
      "source": [
        "# removing stopwords\n",
        "stop_words = ENGLISH_STOP_WORDS\n",
        "\n",
        "def remove_stopwords(text):\n",
        "    words = [word for word in text if word not in stop_words]\n",
        "    return words \n",
        "df['stopwords_remove_tokens'] = df['tokens'].apply(lambda x : remove_stopwords(x))\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5cecb886",
      "metadata": {
        "id": "4b60aee3"
      },
      "source": [
        "### Lemmatization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a78781dc",
      "metadata": {
        "id": "e1c191cf"
      },
      "outputs": [],
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "lem = WordNetLemmatizer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "83a60720",
      "metadata": {
        "id": "ba0720d7",
        "outputId": "ce14c7fc-0650-45b9-eb91-14dcf0ff1b98"
      },
      "outputs": [],
      "source": [
        "def lem_word(x):\n",
        "    return [lem.lemmatize(w, pos=\"v\") for w in x]\n",
        "\n",
        "df['lemmatized_text'] = df['stopwords_remove_tokens'].apply(lem_word)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "abdae886",
      "metadata": {
        "id": "a0f84955"
      },
      "source": [
        "### Combine the text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1eed091a",
      "metadata": {
        "id": "45eb60df",
        "outputId": "8dbe5f80-a87d-4fa8-ce8f-49908f3209ae",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "def combine_text(list_of_text):\n",
        "    combined_text = ' '.join(list_of_text)\n",
        "    return combined_text\n",
        "\n",
        "df['final_text'] = df['lemmatized_text'].apply(lambda x : combine_text(x))\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9c914fc9",
      "metadata": {
        "id": "efcfe23f",
        "outputId": "5d022202-3f4b-4735-b908-0836979c392a"
      },
      "outputs": [],
      "source": [
        "df['final_number_of_words'] = df['final_text'].apply(lambda x:len(str(x).split()))\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "61695805",
      "metadata": {
        "id": "5a26b7b6",
        "outputId": "fb0a275e-8781-4d31-d7aa-84ba3ac79be2"
      },
      "outputs": [],
      "source": [
        "df['final_number_of_words'].describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "df5a2d6c",
      "metadata": {
        "id": "3e96822c",
        "outputId": "9a136d8c-e19e-427a-f435-8c86e5351743"
      },
      "outputs": [],
      "source": [
        "# number of rows with text length = 0\n",
        "no_text = df[df['final_number_of_words'] == 0]\n",
        "print(len(no_text))\n",
        "\n",
        "# drop these rows\n",
        "df.drop(no_text.index,inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "26fda6a7",
      "metadata": {
        "id": "73460ccf"
      },
      "source": [
        "### Visualize the frequency distribution of number of words for each text extracted"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a8d4d6b4",
      "metadata": {
        "id": "2059838c",
        "outputId": "bd39eb6d-9fc6-4e59-e624-6b3a96430180"
      },
      "outputs": [],
      "source": [
        "plt.style.use('ggplot')\n",
        "plt.figure(figsize=(12,6))\n",
        "sns.distplot(df['final_number_of_words'], kde = False,color=\"red\", bins=200)\n",
        "plt.title(\"Frequency distribution of number of words for each text extracted\", size=20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "81fe2238",
      "metadata": {
        "id": "ef2dbed1"
      },
      "outputs": [],
      "source": [
        "df.to_csv(data_root_path + \"data.csv\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e2c4f256",
      "metadata": {
        "id": "dc189836"
      },
      "source": [
        "# Feature extraction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b05cca94",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XGzBTgrUHOIi",
        "outputId": "2606eaad-37e2-4049-c998-15f02beaf0df"
      },
      "outputs": [],
      "source": [
        "cd /content/drive/MyDrive/ML-Projects-MidTerm/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "491a2b2b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nGKKDoyXHXUP",
        "outputId": "66399a47-7834-4cfe-84e9-3a2db20f3042"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "data_path = os.getcwd() + '/data/data.csv'\n",
        "data_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cb496265",
      "metadata": {
        "id": "JXCZZBCUHeK4"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(data_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3906aa81",
      "metadata": {
        "id": "6c3ca06e"
      },
      "source": [
        "### Our text has been cleaned, we will convert the labels into numeric values using LableEncoder()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "acaed0f5",
      "metadata": {
        "id": "8244ec0e"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import LabelEncoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "be7bd872",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a3231e52",
        "outputId": "b22e8a48-3bc2-4a18-b941-f5495bf7c22f"
      },
      "outputs": [],
      "source": [
        "# label_encoder object knows how to understand word labels.\n",
        "label_encoder = LabelEncoder()\n",
        "  \n",
        "# Encode labels in column 'label'.\n",
        "df['target']= label_encoder.fit_transform(df['label'])\n",
        "  \n",
        "df['target'].unique()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7025127d",
      "metadata": {
        "id": "ecf4c4b5"
      },
      "source": [
        "Variable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a3a536dc",
      "metadata": {
        "id": "801b6911"
      },
      "outputs": [],
      "source": [
        "X = df['final_text']\n",
        "y = df['target']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8177e492",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8f359109",
        "outputId": "68b974e7-de38-44ce-d30a-e385987bbfaa"
      },
      "outputs": [],
      "source": [
        "X.shape, y.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2f019ed1",
      "metadata": {
        "id": "493b7c5b"
      },
      "source": [
        "Split the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "001f389a",
      "metadata": {
        "id": "f4b5ee3d"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "223dade1",
      "metadata": {
        "id": "1b7701a6"
      },
      "outputs": [],
      "source": [
        "X_data, X_test, y_data, y_test = train_test_split(X, y, test_size=0.25)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e8f9b79f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9af29c79",
        "outputId": "36500ec3-32aa-4385-92b5-6edadc8ce968"
      },
      "outputs": [],
      "source": [
        "X_data.shape, X_test.shape, y_data.shape, y_test.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "badc6195",
      "metadata": {
        "id": "7206cc86"
      },
      "source": [
        "### TF - IDF"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "84fb794f",
      "metadata": {
        "id": "a989201a"
      },
      "source": [
        "Tf-Idf stands for Term Frequency-Inverse Document Frequency. It is a techinque to quantify a word in documents, we generally compute a weight to each word which signifies the importance of the word which signifies the importance of the word in the document and corpus."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "727e8301",
      "metadata": {
        "id": "05107a93"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "34371522",
      "metadata": {
        "id": "77042dbe"
      },
      "source": [
        "TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "540b4b19",
      "metadata": {
        "id": "6e8ce147"
      },
      "outputs": [],
      "source": [
        "tfidf_vectorizer = TfidfVectorizer(analyzer='word',stop_words='english', max_df=0.8, min_df=3, sublinear_tf=True)\n",
        "# tfidf_vectorizer = CountVectorizer(analyzer='word',stop_words='english', max_df=0.8, min_df=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "324bc281",
      "metadata": {
        "id": "74518773"
      },
      "outputs": [],
      "source": [
        "tfidf_vectorizer.fit(X_data)\n",
        "X_data = tfidf_vectorizer.transform(X_data)\n",
        "X_test = tfidf_vectorizer.transform(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "24f82688",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cedce5ed",
        "outputId": "561bcd18-6723-4dd7-91ff-fa84e889eb99"
      },
      "outputs": [],
      "source": [
        "X_data.shape, X_test.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "33290107",
      "metadata": {
        "id": "f04b4b54"
      },
      "source": [
        "Check the vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4231ed2b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3b7d64c6",
        "outputId": "63924958-7e5d-43ae-a152-b126846d87a3"
      },
      "outputs": [],
      "source": [
        "len(tfidf_vectorizer.vocabulary_)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "018d21dd",
      "metadata": {
        "id": "4598374f"
      },
      "source": [
        "After performing TF-IDF, we can easily see that the matrix we obtained has a very large size, and the computational processing with this matrix requires quite expensive time and memory. To handle this problem, we will use the SVD (singular value decomposition) algorithm, which aims to reduce the data dimension of the resulting matrix, while keeping the properties of the original matrix."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "463ce65e",
      "metadata": {
        "id": "aa482357"
      },
      "outputs": [],
      "source": [
        "from sklearn.decomposition import TruncatedSVD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7ac74fd8",
      "metadata": {
        "id": "2ef9989a"
      },
      "outputs": [],
      "source": [
        "svd = TruncatedSVD(n_components=300)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3f47abf0",
      "metadata": {
        "id": "202c1bd0"
      },
      "outputs": [],
      "source": [
        "svd.fit(X_data)\n",
        "X_data = svd.transform(X_data)\n",
        "X_test = svd.transform(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0756e460",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8408bc25",
        "outputId": "c36bc5ef-9acc-4b64-a2a9-8e25a5798558"
      },
      "outputs": [],
      "source": [
        "X_data.shape, X_test.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4a4bbd43",
      "metadata": {
        "id": "e3a53182"
      },
      "source": [
        "# Build the model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7401d3e9",
      "metadata": {
        "id": "5afcd9d8"
      },
      "source": [
        "Firstly, we create containers to save time and accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "11b79a83",
      "metadata": {
        "id": "b484963d"
      },
      "outputs": [],
      "source": [
        "training_time_container = {'linear_clf':0, 'knn':0,'ann':0}\n",
        "prediction_time_container = {'linear_clf':0, 'knn':0,'ann':0}\n",
        "accuracy_container = {'linear_clf':{'train': 0, 'val':0,'test':0},'knn':{'train': 0, 'val':0,'test':0},\n",
        "                      'ann':{'train': 0, 'val':0,'test':0}}\n",
        "f1_macro_container = {'linear_clf':{'train': 0, 'val':0,'test':0},'knn':{'train': 0, 'val':0,'test':0},\n",
        "                      'ann':{'train': 0, 'val':0,'test':0}}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c4d3d6db",
      "metadata": {
        "id": "30aafe83"
      },
      "source": [
        "In order to keep the code concise, we will use the same training and prediction function for all models, which greatly reduces our coding time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "94e95195",
      "metadata": {
        "code_folding": [],
        "id": "88d70d7e"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers import Dense, Input, Dropout\n",
        "from tensorflow.keras import models, optimizers\n",
        "from tensorflow.keras.callbacks import EarlyStopping \n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix, f1_score, classification_report\n",
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "from time import time\n",
        "import numpy as np\n",
        "\n",
        "# train_model function\n",
        "def train_model(model, X_data, y_data, X_test, y_test, model_name, \n",
        "                is_neuralnet=False, epochs=3, batch_size=128, show_cm=False):       \n",
        "    X_train, X_val, y_train, y_val = train_test_split(X_data, y_data,\n",
        "                                test_size=0.2, random_state=42)\n",
        "    \n",
        "    if is_neuralnet:\n",
        "        earlystop = EarlyStopping(monitor='loss', min_delta=0, patience=3, verbose=0, mode='auto')\n",
        "        # Fitting the model \n",
        "        start_train_time = time()\n",
        "        history = model.fit(X_train, y_train, validation_data=(X_val, y_val),\n",
        "                            epochs=epochs, batch_size=batch_size,callbacks=[earlystop])\n",
        "        training_time_container[model_name] = round(time() - start_train_time,4)\n",
        "        \n",
        "        # Predicting the Test and Val set results\n",
        "        start_predict_time = time()\n",
        "        # Predict probability\n",
        "        train_predictions = model.predict(X_train)\n",
        "        val_predictions = model.predict(X_val)\n",
        "        test_predictions = model.predict(X_test)\n",
        "        # Get predict\n",
        "        train_predictions = train_predictions.argmax(axis=-1)\n",
        "        val_predictions = val_predictions.argmax(axis=-1)\n",
        "        test_predictions = test_predictions.argmax(axis=-1)\n",
        "        prediction_time_container[model_name] = round(time() - start_predict_time,4)\n",
        "\n",
        "        acc = history.history['accuracy']\n",
        "        val_acc = history.history['val_accuracy']\n",
        "        loss = history.history['loss']\n",
        "        val_loss = history.history['val_loss']\n",
        "\n",
        "        epochs = range(len(acc))\n",
        "\n",
        "        plt.plot(epochs, acc, 'r', label='Training accuracy')\n",
        "        plt.plot(epochs, val_acc, 'b', label='Validation accuracy')\n",
        "        plt.title('Training and validation accuracy')\n",
        "        plt.legend()\n",
        "        plt.figure()\n",
        "\n",
        "        plt.plot(epochs, loss, 'r', label='Training Loss')\n",
        "        plt.plot(epochs, val_loss, 'b', label='Validation Loss')\n",
        "        plt.title('Training and validation loss')\n",
        "        plt.legend()\n",
        "\n",
        "        plt.show()\n",
        "        \n",
        "    else:\n",
        "        # Fitting the model \n",
        "        start_train_time = time()\n",
        "        model.fit(X_train, y_train)\n",
        "        training_time_container[model_name] = round(time() - start_train_time,4)\n",
        "\n",
        "        # Predicting the Test and Val set results\n",
        "        start_predict_time = time()\n",
        "        train_predictions = model.predict(X_train)\n",
        "        val_predictions = model.predict(X_val)\n",
        "        test_predictions = model.predict(X_test)\n",
        "        prediction_time_container[model_name] = round(time() - start_predict_time,4)\n",
        "    \n",
        "    train_acc = accuracy_score(y_train, train_predictions)\n",
        "    accuracy_container[model_name]['train'] = round(train_acc,4)\n",
        "    \n",
        "    val_acc = accuracy_score(y_val, val_predictions)\n",
        "    accuracy_container[model_name]['val'] = round(val_acc,4)\n",
        "    \n",
        "    test_acc = accuracy_score(y_test, test_predictions)\n",
        "    accuracy_container[model_name]['test'] = round(test_acc,4)\n",
        "    \n",
        "    f1_train = f1_score(y_train, train_predictions, average='macro')\n",
        "    f1_macro_container[model_name]['train'] = round(f1_train, 4)\n",
        "\n",
        "    f1_val = f1_score(y_val, val_predictions, average='macro')\n",
        "    f1_macro_container[model_name]['val'] = round(f1_val,4)\n",
        "\n",
        "    f1_test = f1_score(y_test, test_predictions, average='macro')\n",
        "    f1_macro_container[model_name]['test'] = round(f1_test,4)\n",
        "    \n",
        "    print(\"Training accuracy: \" + str(train_acc*100) + \"%\")\n",
        "    print(\"Validation accuracy: \" + str(val_acc*100) + \"%\")\n",
        "    print(\"Testing accuracy: \" + str(test_acc*100) + \"%\")\n",
        "    print(\"Training F1-macro: \", str(f1_train*100) + \"%\")\n",
        "    print(\"Validation F1-macro: \", str(f1_train*100) + \"%\")\n",
        "    print(\"Testing F1-macro: \", str(f1_test*100) + \"%\")\n",
        "    print(\"Training time: \" + str(training_time_container[model_name]) + 's')\n",
        "    print(\"Prediction time: \" + str(prediction_time_container[model_name]) + 's')\n",
        "\n",
        "    \n",
        "    # classification report\n",
        "    df_report = pd.DataFrame(classification_report(y_test, test_predictions, output_dict=True)).transpose()\n",
        "    \n",
        "    # plot the confusion matrix\n",
        "    if show_cm:\n",
        "        cm = confusion_matrix(y_test, test_predictions)\n",
        "        disp = ConfusionMatrixDisplay(confusion_matrix=cm,\n",
        "                                      display_labels=label_encoder.classes_)\n",
        "\n",
        "        # NOTE: Fill all variables here with default values of the plot_confusion_matrix\n",
        "        plt.style.use('default')\n",
        "        fig, ax = plt.subplots(figsize=(10, 10))\n",
        "        disp = disp.plot(xticks_rotation='vertical', ax=ax, cmap='summer')\n",
        "\n",
        "        plt.show()\n",
        "    return df_report"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "158d31ae",
      "metadata": {
        "id": "70d4ad3f"
      },
      "source": [
        "# K-nearest Neighbor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2f40ba53",
      "metadata": {
        "id": "cb43e4ca"
      },
      "outputs": [],
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "651ec913",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 847
        },
        "id": "8a0dd3e4",
        "outputId": "e47b30f3-c486-4120-84cc-15128966146e",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "# On TF-IDF\n",
        "train_model(KNeighborsClassifier() , X_data, y_data, X_test, y_test, model_name='knn',show_cm=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b7335ae6",
      "metadata": {
        "id": "991d1e85"
      },
      "source": [
        "GridSearch for K nearest Neighbor Algorithm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f2d73c31",
      "metadata": {
        "id": "ace5a1b5",
        "outputId": "e911d54d-0189-478e-82f6-b8ba0e9181b4",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import hamming_loss, make_scorer\n",
        "\n",
        "k_range = list(range(1,15,2))\n",
        "weight_options = [\"uniform\", \"distance\"]\n",
        "\n",
        "param_grid = dict(n_neighbors = k_range, weights = weight_options)\n",
        "knn = KNeighborsClassifier()\n",
        "\n",
        "grid = GridSearchCV(knn, param_grid, scoring = 'f1_macro', return_train_score=True)\n",
        "grid.fit(X_data, y_data)\n",
        "\n",
        "# print(grid.return_train_score)\n",
        "# print(grid.cv_results_)\n",
        "results = pd.DataFrame(grid.cv_results_)\n",
        "print(results)\n",
        "print (grid.best_score_)\n",
        "print (grid.best_params_)\n",
        "print (grid.best_estimator_)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5b0e5522",
      "metadata": {
        "id": "c7bb6e10",
        "outputId": "ac248562-1444-441d-c330-26dd6621edcd"
      },
      "outputs": [],
      "source": [
        "score_metric=\"f1_macro\"\n",
        "best = np.argmax(results.mean_test_score.values)\n",
        "cv = 5\n",
        "print(\"\\n------Plotting Cross-Validated Grid Search Results--------\\n\")\n",
        "def plot_Xvalidated_grid(results,grid_classifier):\n",
        "    plt.figure(figsize=(10, 3))\n",
        "    plt.xlim(-1, len(results))\n",
        "\n",
        "    for i, (_, row) in enumerate(results.iterrows()):\n",
        "        scores = row[['split%d_test_score' % i for i in range(cv)]]\n",
        "        marker_cv, = plt.plot([i] * cv, scores, '^', c='gray', markersize=5,\n",
        "                                  alpha=.5)\n",
        "        marker_mean, = plt.plot(i, row.mean_test_score, 'v', c='none', alpha=1,\n",
        "                                    markersize=10, markeredgecolor='k')\n",
        "        if i == best:\n",
        "            marker_best, = plt.plot(i, row.mean_test_score, 'o', c='red',\n",
        "                                        fillstyle=\"none\", alpha=1, markersize=20,\n",
        "                                        markeredgewidth=3)\n",
        "\n",
        "    plt.xticks(range(len(results)), [str(x).strip(\"{}\").replace(\"'\", \"\") for x\n",
        "                            in grid_classifier.cv_results_['params']],rotation=90)\n",
        "    plt.ylabel(\"Validation score\")\n",
        "    plt.xlabel(\"Parameter settings\")\n",
        "    plt.legend([marker_cv, marker_mean, marker_best],\n",
        "                   [\"cv \"+score_metric, \"mean \"+score_metric, \"best parameter setting\"],\n",
        "                   loc=\"best\")\n",
        "    plt.show()\n",
        "\n",
        "plot_Xvalidated_grid(results,grid)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "91ee2d82",
      "metadata": {
        "id": "bfcd2cce",
        "outputId": "f59171fd-2f78-4f65-f417-093d7ba2bb2e"
      },
      "outputs": [],
      "source": [
        "train_model(grid.best_estimator_, X_data, y_data, X_test, y_test, model_name='knn', show_cm=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e09f5fd1",
      "metadata": {
        "id": "e6aa8254"
      },
      "source": [
        "# Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ee8e9a74",
      "metadata": {
        "id": "0bb6cf9c"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2e5da036",
      "metadata": {
        "id": "b30aee34"
      },
      "outputs": [],
      "source": [
        "scaler = StandardScaler()\n",
        "scaler.fit(X_data)\n",
        "X_data_scaler = scaler.transform(X_data)\n",
        "X_test_scaler = scaler.transform(X_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d34189d8",
      "metadata": {
        "id": "61281225",
        "outputId": "b7789bb1-7ccb-4fd5-9658-814a97470753",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "# On TF-IDF\n",
        "model = LogisticRegression(max_iter=1000, multi_class='ovr')\n",
        "train_model(model , X_data_scaler, y_data, X_test_scaler, y_test, model_name='linear_clf',show_cm=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dbbf8f9f",
      "metadata": {
        "id": "410b3d65"
      },
      "source": [
        "GridSearch for Linear Classifier Algorithm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b6cf4e87",
      "metadata": {
        "id": "48f60f70",
        "outputId": "6420887f-7d93-470d-fede-62feac1649c8",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "C_options = [0.0001, 0.001, 0.01, 0.1, 1, 10, 100]\n",
        "\n",
        "param_grid = dict(C=C_options)\n",
        "logistic_reg = LogisticRegression(max_iter=1000 ,penalty='l2', multi_class='ovr')\n",
        "\n",
        "grid = GridSearchCV(logistic_reg, param_grid, scoring = 'f1_macro', return_train_score=True)\n",
        "grid.fit(X_data_scaler, y_data)\n",
        "\n",
        "results = pd.DataFrame(grid.cv_results_)\n",
        "print(results)\n",
        "print (grid.best_score_)\n",
        "print (grid.best_params_)\n",
        "print (grid.best_estimator_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1d43c0e6",
      "metadata": {
        "id": "fc1b54bf",
        "outputId": "898f5858-d346-44ad-fecb-5d51a86f800e"
      },
      "outputs": [],
      "source": [
        "score_metric=\"f1_macro\"\n",
        "best = np.argmax(results.mean_test_score.values)\n",
        "cv = 5\n",
        "print(\"\\n------Plotting Cross-Validated Grid Search Results--------\\n\")\n",
        "def plot_Xvalidated_grid(results,grid_classifier):\n",
        "    plt.figure(figsize=(10, 3))\n",
        "    plt.xlim(-1, len(results))\n",
        "\n",
        "    for i, (_, row) in enumerate(results.iterrows()):\n",
        "        scores = row[['split%d_test_score' % i for i in range(cv)]]\n",
        "        marker_cv, = plt.plot([i] * cv, scores, '^', c='gray', markersize=5,\n",
        "                                  alpha=.5)\n",
        "        marker_mean, = plt.plot(i, row.mean_test_score, 'v', c='none', alpha=1,\n",
        "                                    markersize=10, markeredgecolor='k')\n",
        "        if i == best:\n",
        "            marker_best, = plt.plot(i, row.mean_test_score, 'o', c='red',\n",
        "                                        fillstyle=\"none\", alpha=1, markersize=20,\n",
        "                                        markeredgewidth=3)\n",
        "\n",
        "    plt.xticks(range(len(results)), [str(x).strip(\"{}\").replace(\"'\", \"\") for x\n",
        "                            in grid_classifier.cv_results_['params']],rotation=90)\n",
        "    plt.ylabel(\"Validation score\")\n",
        "    plt.xlabel(\"Parameter settings\")\n",
        "    plt.legend([marker_cv, marker_mean, marker_best],\n",
        "                   [\"cv \"+score_metric, \"mean \"+score_metric, \"best parameter setting\"],\n",
        "                   loc=\"best\")\n",
        "    plt.show()\n",
        "\n",
        "plot_Xvalidated_grid(results,grid)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "59cc7dcd",
      "metadata": {
        "id": "e08f16fa",
        "outputId": "8c57a303-a3d5-494d-da0e-8061708ecf26"
      },
      "outputs": [],
      "source": [
        "train_model(grid.best_estimator_, X_data_scaler, y_data, X_test_scaler, y_test, model_name='linear_clf', show_cm=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cb0afe31",
      "metadata": {
        "id": "74Fb5s4nJYTE"
      },
      "source": [
        "# Artificial Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4c478cb4",
      "metadata": {
        "id": "43e411f1"
      },
      "outputs": [],
      "source": [
        "def create_ann_model(input_shape):\n",
        "    input_layer = Input(shape=(input_shape,))\n",
        "    layer = Dense(512, activation='relu')(input_layer)\n",
        "    layer = Dense(256, activation='relu')(layer)\n",
        "    layer = Dense(256, activation='relu')(layer)\n",
        "    output_layer = Dense(20, activation='softmax')(layer)\n",
        "    \n",
        "    classifier = models.Model(input_layer, output_layer)\n",
        "    classifier.compile(optimizer=optimizers.Adam(), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "    \n",
        "    return classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fc59d7fa",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "R2rE8CCnIf2a",
        "outputId": "42c269a0-0087-4f61-af8f-00fd1711cf87",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "train_model(create_ann_model(X_data.shape[1]), X_data, y_data.values, X_test, y_test.values, model_name = 'ann', is_neuralnet=True, epochs=3,show_cm=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ab2a3e89",
      "metadata": {
        "id": "cEP_UEi6l4Wc"
      },
      "source": [
        "Add Dropout layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3da826c8",
      "metadata": {
        "id": "0jfTmlnol-ML"
      },
      "outputs": [],
      "source": [
        "def create_ann_model(input_shape):\n",
        "    input_layer = Input(shape=(input_shape,))\n",
        "    layer = Dense(512, activation='relu')(input_layer)\n",
        "    layer = Dropout(0.5)(layer)\n",
        "    layer = Dense(256, activation='relu')(layer)\n",
        "    layer = Dropout(0.5)(layer)\n",
        "    layer = Dense(256, activation='relu')(layer)\n",
        "    output_layer = Dense(20, activation='softmax')(layer)\n",
        "    \n",
        "    classifier = models.Model(input_layer, output_layer)\n",
        "    classifier.compile(optimizer=optimizers.Adam(), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "    \n",
        "    return classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "67f6eb0c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "HgTock9Hl-vI",
        "outputId": "281185c0-a860-4578-e2b5-51192edb2371",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "train_model(create_ann_model(X_data.shape[1]), X_data, y_data.values, X_test, y_test.values, model_name = 'ann', is_neuralnet=True, epochs=7,show_cm=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "170b6b5e",
      "metadata": {
        "id": "J0AFAFRMaTxE"
      },
      "source": [
        "# Result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "78150f40",
      "metadata": {
        "id": "1494ca64"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe kernel failed to start due to the missing module 'ipykernel'. Consider installing this module. \n",
            "View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details.\n",
            "<a href='https://aka.ms/kernelFailuresMissingModule'>Learn more</a>"
          ]
        }
      ],
      "source": [
        "def convert_result(training_time_container, prediction_time_container, accuracy_container):    \n",
        "    df_result = pd.DataFrame(columns=[\"Model name\", \"Train Score\", \"Validation Score\", \"Test Score\",\"F1 macro train\", \"F1 macro val\", \"F1 macro test\", \"Training Time\", \"Prediction Time\"])\n",
        "    for model in training_time_container:\n",
        "        df_result = df_result.append({\n",
        "            \"Model name\": model,\n",
        "            \"Train Score\": accuracy_container[model]['train']*100,\n",
        "            \"Validation Score\": accuracy_container[model]['val']*100,\n",
        "            \"Test Score\": accuracy_container[model]['test']*100,\n",
        "            \"F1 macro train\": f1_macro_container[model]['train']*100,\n",
        "            \"F1 macro val\": f1_macro_container[model]['val']*100,\n",
        "            \"F1 macro test\": f1_macro_container[model]['test']*100,\n",
        "            \"Training Time\": str(training_time_container[model]) + 's',\n",
        "            \"Prediction Time\": str(prediction_time_container[model]) + 's'\n",
        "        }, ignore_index=True)\n",
        "    return df_result\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f8cb1bbe",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "id": "fe78596b",
        "outputId": "ebdfa169-e305-44bb-e4d7-81e2a22d1e27"
      },
      "outputs": [],
      "source": [
        "convert_result(training_time_container, prediction_time_container, accuracy_container)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4a93f02c",
      "metadata": {
        "id": "8bc3ad7b"
      },
      "outputs": [],
      "source": [
        "import plotly.graph_objects as go"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1ce5eca4",
      "metadata": {
        "id": "cb03bcf5",
        "outputId": "e0232d65-dce8-4b9b-ac56-9b6ab7bbcefa"
      },
      "outputs": [],
      "source": [
        "data = list(accuracy_container.values())\n",
        "fig=go.Figure(data=[go.Bar(name = 'train', x =list(accuracy_container.keys()), y = [data[i]['train'] for i in range(len(data))], text =[data[i]['train'] for i in range(len(data))], textposition='auto' ),\n",
        "                    go.Bar(name = 'val', x =list(accuracy_container.keys()), y = [data[i]['val'] for i in range(len(data))], text =[data[i]['val'] for i in range(len(data))], textposition='auto'),\n",
        "                    go.Bar(name = 'test', x =list(accuracy_container.keys()), y = [data[i]['test'] for i in range(len(data))], text =[data[i]['test'] for i in range(len(data))], textposition='auto')])\n",
        "                    \n",
        "\n",
        "fig.update_layout(autosize=True ,plot_bgcolor='rgb(275, 275, 275)',\n",
        "                  title=\"Comparison of Accuracy Scores of different classifiers\",\n",
        "                    xaxis_title=\"Machine Learning Models\",\n",
        "                    yaxis_title=\"Accuracy Scores\" ,\n",
        "                 barmode = 'group')\n",
        "\n",
        "fig.data[0].marker.line.width = 3\n",
        "fig.data[0].marker.line.color = \"white\"  \n",
        "fig"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6386fd23",
      "metadata": {
        "id": "137b819e",
        "outputId": "b302ec02-0b25-4a2a-c862-5dd76bbd20f6"
      },
      "outputs": [],
      "source": [
        "data = list(f1_macro_container.values())\n",
        "fig=go.Figure(data=[go.Bar(name = 'train', x =list(f1_macro_container.keys()), y = [data[i]['train'] for i in range(len(data))], text =[data[i]['train'] for i in range(len(data))], textposition='auto'),\n",
        "                    go.Bar(name = 'val', x =list(f1_macro_container.keys()), y = [data[i]['val'] for i in range(len(data))], text =[data[i]['val'] for i in range(len(data))], textposition='auto'),\n",
        "                    go.Bar(name = 'test', x =list(f1_macro_container.keys()), y = [data[i]['test'] for i in range(len(data))], text =[data[i]['test'] for i in range(len(data))], textposition='auto')])\n",
        "                    \n",
        "\n",
        "fig.update_layout(autosize=True ,plot_bgcolor='rgb(275, 275, 275)',\n",
        "                  title=\"Comparison of Marco-Average F1-score of different classifiers\",\n",
        "                  xaxis_title=\"Machine Learning Models\",\n",
        "                  yaxis_title=\"Macro-F1 Scores\" ,\n",
        "                  barmode = 'group')\n",
        "\n",
        "fig.data[0].marker.line.width = 3\n",
        "fig.data[0].marker.line.color = \"white\"  \n",
        "fig"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bf4964d3",
      "metadata": {
        "id": "02ebbb9c",
        "outputId": "8637529d-228e-4382-c18f-68548bb907a0"
      },
      "outputs": [],
      "source": [
        "fig=go.Figure(data=[go.Bar(y=list(training_time_container.values()),x=list(training_time_container.keys()),\n",
        "                           marker={'color':np.arange(len(list(training_time_container.values())))}\n",
        "                          ,text=list(training_time_container.values()), textposition='auto' )])\n",
        "\n",
        "fig.update_layout(autosize=True ,plot_bgcolor='rgb(275, 275, 275)',\n",
        "                  title=\"Comparison of Training Time of different classifiers\",\n",
        "                    xaxis_title=\"Machine Learning Models\",\n",
        "                    yaxis_title=\"Training time in seconds\" )\n",
        "\n",
        "fig.data[0].marker.line.width = 3\n",
        "fig.data[0].marker.line.color = \"black\"  \n",
        "fig"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "59328a6f",
      "metadata": {
        "id": "9c5c1ecd",
        "outputId": "fdd7839f-fb73-43e6-a0d6-6452ae2a3fd3"
      },
      "outputs": [],
      "source": [
        "fig=go.Figure(data=[go.Bar(y=list(prediction_time_container.values()),x=list(prediction_time_container.keys()),\n",
        "                           marker={'color':np.arange(len(list(prediction_time_container.values())))}\n",
        "                          ,text=list(prediction_time_container.values()), textposition='auto' )])\n",
        "\n",
        "fig.update_layout(autosize=True ,plot_bgcolor='rgb(275, 275, 275)',\n",
        "                  title=\"Comparison of Prediction Time of different classifiers\",\n",
        "                    xaxis_title=\"Machine Learning Models\",\n",
        "                    yaxis_title=\"Prediction time in seconds\")\n",
        "\n",
        "fig.data[0].marker.line.width = 3\n",
        "fig.data[0].marker.line.color = \"black\"  \n",
        "fig"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "878ebf7a",
      "metadata": {
        "id": "67a51fcb"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "d5eddf1e",
        "4e5744ec",
        "8c663152",
        "RHueTS-RZJyW",
        "4b60aee3",
        "a0f84955",
        "73460ccf",
        "dc189836",
        "J0AFAFRMaTxE"
      ],
      "name": "20_newsgroups_classification.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "5a4dfa0490902d4f83d8b54e629d7e6e6fc5d123af77ba9d2da780b131b1b747"
    },
    "kernelspec": {
      "display_name": "Python 3.6.4 32-bit",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": ""
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
